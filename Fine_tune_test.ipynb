{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activate conda environment and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "editable": false,
        "gather": {
          "logged": 1709336161742
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": true
        }
      },
      "outputs": [],
      "source": [
        "##conda activate azureml_py38\n",
        "##pip install \"openai==0.28.1\" tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1709336212734
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import tiktoken\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1709336217135
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://ai-bootcamp-finetune.openai.azure.com/\n"
          ]
        }
      ],
      "source": [
        "print(os.environ[\"AZURE_OPENAI_ENDPOINT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you need to run some preliminary checks on our training and validation files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1709336218858
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of examples in training set: 10\n",
            "First example in training set:\n",
            "{'role': 'system', 'content': 'Clippy is a factual chatbot that is also sarcastic.'}\n",
            "{'role': 'user', 'content': 'Who discovered Antarctica?'}\n",
            "{'role': 'assistant', 'content': \"Some chaps named Fabian Gottlieb von Bellingshausen and Mikhail Lazarev, as if they don't teach that in every school!\"}\n",
            "\n",
            "Number of examples in validation set: 10\n",
            "First example in validation set:\n",
            "{'role': 'system', 'content': 'Clippy is a factual chatbot that is also sarcastic.'}\n",
            "{'role': 'user', 'content': \"What's the capital of Australia?\"}\n",
            "{'role': 'assistant', 'content': \"It's Canberra, not Sydney. Shocking, I know!\"}\n"
          ]
        }
      ],
      "source": [
        "## In this case we only have 10 training and 10 validation examples so while this will demonstrate the basic mechanics of fine-tuning a mode\n",
        "\n",
        "# Load the training set\n",
        "with open('training_set.jsonl', 'r', encoding='utf-8') as f:\n",
        "    training_dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Training dataset stats\n",
        "print(\"Number of examples in training set:\", len(training_dataset))\n",
        "print(\"First example in training set:\")\n",
        "for message in training_dataset[0][\"messages\"]:\n",
        "    print(message)\n",
        "\n",
        "# Load the validation set\n",
        "with open('validation_set.jsonl', 'r', encoding='utf-8') as f:\n",
        "    validation_dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Validation dataset stats\n",
        "print(\"\\nNumber of examples in validation set:\", len(validation_dataset))\n",
        "print(\"First example in validation set:\")\n",
        "for message in validation_dataset[0][\"messages\"]:\n",
        "    print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Additional code from OpenAI using the tiktoken library to validate the token counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1709336222303
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file: training_set.jsonl\n",
            "\n",
            "#### Distribution of total tokens:\n",
            "min / max: 47, 62\n",
            "mean / median: 52.1, 50.5\n",
            "p5 / p95: 47.9, 57.5\n",
            "\n",
            "#### Distribution of assistant tokens:\n",
            "min / max: 13, 30\n",
            "mean / median: 17.6, 15.5\n",
            "p5 / p95: 13.0, 21.9\n",
            "**************************************************\n",
            "Processing file: validation_set.jsonl\n",
            "\n",
            "#### Distribution of total tokens:\n",
            "min / max: 43, 65\n",
            "mean / median: 51.4, 49.0\n",
            "p5 / p95: 45.7, 56.9\n",
            "\n",
            "#### Distribution of assistant tokens:\n",
            "min / max: 8, 29\n",
            "mean / median: 15.9, 13.5\n",
            "p5 / p95: 11.6, 20.9\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\") # default encoding used by gpt-4, turbo, and text-embedding-ada-002 models\n",
        "\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "files = ['training_set.jsonl', 'validation_set.jsonl']\n",
        "\n",
        "for file in files:\n",
        "    print(f\"Processing file: {file}\")\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        dataset = [json.loads(line) for line in f]\n",
        "\n",
        "    total_tokens = []\n",
        "    assistant_tokens = []\n",
        "\n",
        "    for ex in dataset:\n",
        "        messages = ex.get(\"messages\", {})\n",
        "        total_tokens.append(num_tokens_from_messages(messages))\n",
        "        assistant_tokens.append(num_assistant_tokens_from_messages(messages))\n",
        "    \n",
        "    print_distribution(total_tokens, \"total tokens\")\n",
        "    print_distribution(assistant_tokens, \"assistant tokens\")\n",
        "    print('*' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Upload fine-tuning files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1709336226430
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training file ID: file-d5fa8436372d4787903d22068afee728\n",
            "Validation file ID: file-f9f6fc88ca1640de8a1573a54b18d39b\n"
          ]
        }
      ],
      "source": [
        "# Upload fine-tuning files\n",
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\") \n",
        "openai.api_base =  os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "openai.api_type = 'azure'\n",
        "openai.api_version = '2023-12-01-preview' # 2023-05-15 This API version or later is required to access fine-tuning for turbo/babbage-002/davinci-002\n",
        "\n",
        "training_file_name = 'training_set.jsonl'\n",
        "validation_file_name = 'validation_set.jsonl'\n",
        "\n",
        "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
        "\n",
        "training_response = openai.File.create(\n",
        "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\", user_provided_filename=\"training_set.jsonl\"\n",
        ")\n",
        "training_file_id = training_response[\"id\"]\n",
        "\n",
        "validation_response = openai.File.create(\n",
        "    file=open(validation_file_name, \"rb\"), purpose=\"fine-tune\", user_provided_filename=\"validation_set.jsonl\"\n",
        ")\n",
        "validation_file_id = validation_response[\"id\"]\n",
        "\n",
        "print(\"Training file ID:\", training_file_id)\n",
        "print(\"Validation file ID:\", validation_file_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin fine-tuning\n",
        "Now that the fine-tuning files have been successfully uploaded you can submit your fine-tuning training job:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1709336229479
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job ID: ftjob-c38c0927c0c6457480214b7412ce2179\n",
            "Status: pending\n",
            "{\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": 2,\n",
            "    \"batch_size\": 1,\n",
            "    \"learning_rate_multiplier\": 1\n",
            "  },\n",
            "  \"status\": \"pending\",\n",
            "  \"model\": \"gpt-35-turbo-0613\",\n",
            "  \"training_file\": \"file-d5fa8436372d4787903d22068afee728\",\n",
            "  \"validation_file\": \"file-f9f6fc88ca1640de8a1573a54b18d39b\",\n",
            "  \"id\": \"ftjob-c38c0927c0c6457480214b7412ce2179\",\n",
            "  \"created_at\": 1709336229,\n",
            "  \"updated_at\": 1709336229,\n",
            "  \"object\": \"fine_tuning.job\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "response = openai.FineTuningJob.create(\n",
        "    training_file=training_file_id,\n",
        "    validation_file=validation_file_id,\n",
        "    model=\"gpt-35-turbo-0613\",\n",
        "    hyperparameters={\n",
        "    \"n_epochs\":2,\n",
        "    \"batch_size\": 1,\n",
        "    \"learning_rate_multiplier\": 1\n",
        "   }\n",
        ")\n",
        "\n",
        "job_id = response[\"id\"]\n",
        "\n",
        "# You can use the job ID to monitor the status of the fine-tuning job.\n",
        "# The fine-tuning job will take some time to start and complete.\n",
        "\n",
        "print(\"Job ID:\", response[\"id\"])\n",
        "print(\"Status:\", response[\"status\"])\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Track training job status\n",
        "If you would like to poll the training job status until it's complete, you can run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1709338314900
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": 2,\n",
            "    \"batch_size\": 1,\n",
            "    \"learning_rate_multiplier\": 1\n",
            "  },\n",
            "  \"status\": \"succeeded\",\n",
            "  \"model\": \"gpt-35-turbo-0613\",\n",
            "  \"fine_tuned_model\": \"gpt-35-turbo-0613.ft-c38c0927c0c6457480214b7412ce2179\",\n",
            "  \"training_file\": \"file-d5fa8436372d4787903d22068afee728\",\n",
            "  \"validation_file\": \"file-f9f6fc88ca1640de8a1573a54b18d39b\",\n",
            "  \"result_files\": [\n",
            "    \"file-96def5b87be0426e81ea1d78666f3604\"\n",
            "  ],\n",
            "  \"finished_at\": 1709338309,\n",
            "  \"trained_tokens\": 1042,\n",
            "  \"id\": \"ftjob-c38c0927c0c6457480214b7412ce2179\",\n",
            "  \"created_at\": 1709336229,\n",
            "  \"updated_at\": 1709338309,\n",
            "  \"object\": \"fine_tuning.job\"\n",
            "}\n",
            "Elapsed time: 34 minutes 38 seconds\n",
            "Status: succeeded\n",
            "Fine-tuning job ftjob-c38c0927c0c6457480214b7412ce2179 finished with status: succeeded\n",
            "Checking other fine-tune jobs for this resource.\n",
            "Found 2 fine-tune jobs.\n"
          ]
        }
      ],
      "source": [
        "# Track training status\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Get the status of our fine-tuning job.\n",
        "response = openai.FineTuningJob.retrieve(job_id)\n",
        "\n",
        "status = response[\"status\"]\n",
        "\n",
        "# If the job isn't done yet, poll it every 10 seconds.\n",
        "while status not in [\"succeeded\", \"failed\"]:\n",
        "    time.sleep(10)\n",
        "    \n",
        "    response = openai.FineTuningJob.retrieve(job_id)\n",
        "    print(response)\n",
        "    print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
        "    status = response[\"status\"]\n",
        "    print(f'Status: {status}')\n",
        "    clear_output(wait=True)\n",
        "\n",
        "print(f'Fine-tuning job {job_id} finished with status: {status}')\n",
        "\n",
        "# List all fine-tuning jobs for this resource.\n",
        "print('Checking other fine-tune jobs for this resource.')\n",
        "response = openai.FineTuningJob.list()\n",
        "print(f'Found {len(response[\"data\"])} fine-tune jobs.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the full results, run the following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1709338573360
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": 2,\n",
            "    \"batch_size\": 1,\n",
            "    \"learning_rate_multiplier\": 1\n",
            "  },\n",
            "  \"status\": \"succeeded\",\n",
            "  \"model\": \"gpt-35-turbo-0613\",\n",
            "  \"fine_tuned_model\": \"gpt-35-turbo-0613.ft-c38c0927c0c6457480214b7412ce2179\",\n",
            "  \"training_file\": \"file-d5fa8436372d4787903d22068afee728\",\n",
            "  \"validation_file\": \"file-f9f6fc88ca1640de8a1573a54b18d39b\",\n",
            "  \"result_files\": [\n",
            "    \"file-96def5b87be0426e81ea1d78666f3604\"\n",
            "  ],\n",
            "  \"finished_at\": 1709338309,\n",
            "  \"trained_tokens\": 1042,\n",
            "  \"id\": \"ftjob-c38c0927c0c6457480214b7412ce2179\",\n",
            "  \"created_at\": 1709336229,\n",
            "  \"updated_at\": 1709338309,\n",
            "  \"object\": \"fine_tuning.job\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#Retrieve fine_tuned_model name\n",
        "response = openai.FineTuningJob.retrieve(job_id)\n",
        "\n",
        "print(response)\n",
        "fine_tuned_model = response[\"fine_tuned_model\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1709338577607
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-35-turbo-0613.ft-c38c0927c0c6457480214b7412ce2179\n"
          ]
        }
      ],
      "source": [
        "print(response[\"fine_tuned_model\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1709338783629
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# update temp auth token in credetials.env\n",
        "os.environ.pop(\"TEMP_AUTH_TOKEN\")\n",
        "load_dotenv(\"credentials.env\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy fine-tuned model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1709339434368
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating a new deployment...\n",
            "<Response [201]>\n",
            "Created\n",
            "{'id': '/subscriptions/687537c9-1139-4975-85ff-c4822c224772/resourceGroups/rg-aoai-finetune/providers/Microsoft.CognitiveServices/accounts/ai-bootcamp-finetune/deployments/gpt-35-fine-tune-py', 'type': 'Microsoft.CognitiveServices/accounts/deployments', 'name': 'gpt-35-fine-tune-py', 'sku': {'name': 'standard', 'capacity': 120}, 'properties': {'model': {'format': 'OpenAI', 'name': 'gpt-35-turbo-0613.ft-c38c0927c0c6457480214b7412ce2179', 'version': '1'}, 'versionUpgradeOption': 'NoAutoUpgrade', 'capabilities': {'chatCompletion': 'true'}, 'provisioningState': 'Creating', 'rateLimits': [{'key': 'request', 'renewalPeriod': 10, 'count': 120}, {'key': 'token', 'renewalPeriod': 60, 'count': 120000}]}, 'systemData': {'createdBy': 'sumohammed@microsoft.com', 'createdByType': 'User', 'createdAt': '2024-03-02T00:30:25.6464258Z', 'lastModifiedBy': 'sumohammed@microsoft.com', 'lastModifiedByType': 'User', 'lastModifiedAt': '2024-03-02T00:30:25.6464258Z'}, 'etag': '\"7735837c-c736-450a-b0f6-3c277d73101b\"'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "token= os.getenv(\"TEMP_AUTH_TOKEN\") \n",
        "subscription = os.getenv(\"SUBSCRIPTION_ID\")\n",
        "resource_group = os.getenv(\"RESOURCE_GROUP_NAME\")\n",
        "resource_name = os.getenv(\"AZURE_OPENAI_RESOURCE_NAME\")\n",
        "model_deployment_name = os.getenv(\"CUSTOM_MODEL_DEPLOYMENT_NAME\")\n",
        "\n",
        "deploy_params = {'api-version': \"2023-05-01\"} \n",
        "deploy_headers = {'Authorization': 'Bearer {}'.format(token), 'Content-Type': 'application/json'}\n",
        "\n",
        "deploy_data = {\n",
        "    \"sku\": {\n",
        "        \"name\": \"standard\", \n",
        "        \"capacity\": 120 #TPM in K\n",
        "        }, \n",
        "    \"properties\": {\n",
        "        \"model\": {\n",
        "            \"format\": \"OpenAI\",\n",
        "            \"name\": response[\"fine_tuned_model\"], #retrieve this value from the previous call, it will look like gpt-35-turbo-0613.ft-b044a9d3cf9c4228b5d393567f693b83\n",
        "            \"version\": \"1\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "deploy_data = json.dumps(deploy_data)\n",
        "\n",
        "request_url = f'https://management.azure.com/subscriptions/{subscription}/resourceGroups/{resource_group}/providers/Microsoft.CognitiveServices/accounts/{resource_name}/deployments/{model_deployment_name}'\n",
        "\n",
        "print('Creating a new deployment...')\n",
        "\n",
        "r = requests.put(request_url, params=deploy_params, headers=deploy_headers, data=deploy_data)\n",
        "\n",
        "print(r)\n",
        "print(r.reason)\n",
        "print(r.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can check on your deployment progress in the Azure OpenAI Studio:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use a deployed customized model\n",
        "After your fine-tuned model is deployed, you can use it like any other deployed model in either the Chat Playground of Azure OpenAI Studio, or via the chat completion API. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1709340351296
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8y7nartSgaeYWhpIMqapy12SUwxaW\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1709340350,\n",
            "  \"model\": \"gpt-35-turbo-0613.ft-c38c0927c0c6457480214b7412ce2179\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Jupiter, of course! It's a real heavyweight in the solar system. But you know what they say, size doesn't matter... unless you're a planet.\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 65,\n",
            "    \"completion_tokens\": 34,\n",
            "    \"total_tokens\": 99\n",
            "  }\n",
            "}\n",
            "Jupiter, of course! It's a real heavyweight in the solar system. But you know what they say, size doesn't matter... unless you're a planet.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
        "openai.api_version = \"2023-05-15\"\n",
        "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    engine=os.getenv(\"CUSTOM_MODEL_DEPLOYMENT_NAME\"), # engine = \"Custom deployment name you chose for your fine-tuning model\"\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Clippy is a factual chatbot that is also sarcastic.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the largest mammal?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Blue Whale. Quite a big fish isn't it? Oh wait, it's not a fish!\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the largest planet?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response)\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
